# -*- coding: utf-8 -*-
"""Another copy of Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mPnTLau-KZHQftuasPCToxc4zcsSKST9
"""



#Mounting google drive to colab

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Setting toolkit folder as working directory

# %cd /content/drive/My Drive/Thesis/Text_Classification_based_on_RNN
! ls

#Importing all the required libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from numpy import array
from keras.preprocessing.text import one_hot, Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import EarlyStopping
#from keras.models import Sequential
from keras.layers import Activation, Dropout, Dense
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from keras.optimizers import Adam
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, precision_score, f1_score
from keras.regularizers import l2
#from keras.optimizers import Adam



#Dataset

movie_reviews = pd.read_csv("Movies.csv", encoding="ISO-8859-1")   #encoding- used while reading the CSV file

movie_reviews.shape

movie_reviews.head(5)

movie_reviews.isnull().values.any()

import seaborn as sns
sns.countplot(x='sentiment', data=movie_reviews)



#Preprocessing is being done here

movie_reviews["review"][2]

TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''

    return TAG_RE.sub('', text)

import nltk
nltk.download('stopwords')

def preprocess_text(sen):
    '''Cleans up text data, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only in lowercase'''

    sentence = sen.lower()

    # Remove html tags
    sentence = remove_tags(sentence)

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Remove multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    # Remove Stopwords
    pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
    sentence = pattern.sub('', sentence)

    return sentence

X = []
sentences = list(movie_reviews['review'])
for sen in sentences:
    X.append(preprocess_text(sen))

X[2]

y = movie_reviews['sentiment']

y = np.array(list(map(lambda x: 1 if x=="positive" else 0, y)))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)



#Embedding Layer

word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(X_train)

X_train = word_tokenizer.texts_to_sequences(X_train)
X_test = word_tokenizer.texts_to_sequences(X_test)

vocab_length = len(word_tokenizer.word_index) + 1

vocab_length

maxlen = 100

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

from numpy import asarray
from numpy import zeros

embeddings_dictionary = dict()
glove_file = open('a2_glove.6B.100d.txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
glove_file.close()

embedding_matrix = zeros((vocab_length, 100))
for word, index in word_tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

embedding_matrix.shape



#Model Training - Feedforward Neural Network (FNN)

#Neural Network architecture

fnn_model = Sequential()
embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)

fnn_model.add(embedding_layer)

fnn_model.add(Flatten())
fnn_model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))
fnn_model.add(Dropout(0.5))

# Output layer
fnn_model.add(Dense(1, activation='sigmoid'))

# Model compiling

optimizer = Adam(learning_rate=0.1)
fnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

print(fnn_model.summary())

fnn_model_history = fnn_model.fit(X_train, y_train, batch_size=128, epochs=8, verbose=1, validation_split=0.2)

score = fnn_model.evaluate(X_test, y_test, verbose=1)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

import matplotlib.pyplot as plt

plt.plot(fnn_model_history.history['acc'])
plt.plot(fnn_model_history.history['val_acc'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(fnn_model_history.history['loss'])
plt.plot(fnn_model_history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Predictions
y_pred = fnn_model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype(int)

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("Confusion Matrix:")
print(conf_matrix)

# Precision
precision = precision_score(y_test, y_pred_classes)
print("Precision:", precision)

# F1 score
f1 = f1_score(y_test, y_pred_classes)
print("F1 Score:", f1)

conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'],
                              index=['True Negative', 'True Positive'])

print("Confusion Matrix:")
print(conf_matrix_df)


# Recall
recall = recall_score(y_test, y_pred_classes)
print("Recall:", recall)


#for micro and macro-average:
# Precision
precision_micro = precision_score(y_test, y_pred_classes, average='micro') # Micro average precision
precision_macro = precision_score(y_test, y_pred_classes, average='macro') # Macro average precision
print("Micro Average Precision:", precision_micro)
print("Macro Average Precision:", precision_macro)

# Recall
recall_micro = recall_score(y_test, y_pred_classes, average='micro') # Micro average recall
recall_macro = recall_score(y_test, y_pred_classes, average='macro') # Macro average recall
print("Micro Average Recall:", recall_micro)
print("Macro Average Recall:", recall_macro)

# F1 score
f1_micro = f1_score(y_test, y_pred_classes, average='micro') # Micro average F1 score
f1_macro = f1_score(y_test, y_pred_classes, average='macro') # Macro average F1 score
print("Micro Average F1 Score:", f1_micro)
print("Macro Average F1 Score:", f1_macro)



#Convolution Neural Network

from keras.layers import Conv1D

cnn_model = Sequential()

embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)
cnn_model.add(embedding_layer)

cnn_model.add(Conv1D(128, 5, activation='relu'))
cnn_model.add(GlobalMaxPooling1D())
cnn_model.add(Dense(1, activation='sigmoid'))

cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(cnn_model.summary())

cnn_model_history = cnn_model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.2)

score = cnn_model.evaluate(X_test, y_test, verbose=1)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

import matplotlib.pyplot as plt

plt.plot(cnn_model_history.history['acc'])
plt.plot(cnn_model_history.history['val_acc'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

plt.plot(cnn_model_history.history['loss'])
plt.plot(cnn_model_history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

# Predictions
y_pred = cnn_model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype(int)

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("Confusion Matrix:")
print(conf_matrix)

# Precision
precision = precision_score(y_test, y_pred_classes)
print("Precision:", precision)

# F1 score
f1 = f1_score(y_test, y_pred_classes)
print("F1 Score:", f1)

conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'],
                              index=['True Negative', 'True Positive'])

print("Confusion Matrix:")
print(conf_matrix_df)

# Recall
recall = recall_score(y_test, y_pred_classes)
print("Recall:", recall)


#for micro and macro-average:
# Precision
precision_micro = precision_score(y_test, y_pred_classes, average='micro') # Micro average precision
precision_macro = precision_score(y_test, y_pred_classes, average='macro') # Macro average precision
print("Micro Average Precision:", precision_micro)
print("Macro Average Precision:", precision_macro)

# Recall
recall_micro = recall_score(y_test, y_pred_classes, average='micro') # Micro average recall
recall_macro = recall_score(y_test, y_pred_classes, average='macro') # Macro average recall
print("Micro Average Recall:", recall_micro)
print("Macro Average Recall:", recall_macro)

# F1 score
f1_micro = f1_score(y_test, y_pred_classes, average='micro') # Micro average F1 score
f1_macro = f1_score(y_test, y_pred_classes, average='macro') # Macro average F1 score
print("Micro Average F1 Score:", f1_micro)
print("Macro Average F1 Score:", f1_macro)



#Recurrant Neural Network

from keras.layers import LSTM

lstm_model = Sequential()
embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)

lstm_model.add(embedding_layer)
lstm_model.add(LSTM(128))

lstm_model.add(Dense(1, activation='sigmoid'))

lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(lstm_model.summary())

lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=8, verbose=1, validation_split=0.2)

score = lstm_model.evaluate(X_test, y_test, verbose=1)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

import matplotlib.pyplot as plt

plt.plot(lstm_model_history.history['acc'])
plt.plot(lstm_model_history.history['val_acc'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(lstm_model_history.history['loss'])
plt.plot(lstm_model_history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

lstm_model.save(f"./c1_lstm_model_acc_{round(score[1], 3)}.h5", save_format='h5')

# Predictions
y_pred = lstm_model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype(int)

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("Confusion Matrix:")      #true values and obtained values to be shown/report
print(conf_matrix)

#print("True Positive (TP):", conf_matrix[1, 1])
#print("False Positive (FP):", conf_matrix[0, 1])
#print("True Negative (TN):", conf_matrix[0, 0])
#print("False Negative (FN):", conf_matrix[1, 0])

conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'],
                              index=['True Negative', 'True Positive'])

print("Confusion Matrix:")
print(conf_matrix_df)

# Precision
precision = precision_score(y_test, y_pred_classes)
print("Precision:", precision)

# F1 score
f1 = f1_score(y_test, y_pred_classes)
print("F1 Score:", f1)

# Recall
recall = recall_score(y_test, y_pred_classes)
print("Recall:", recall)


#for micro and macro-average:
# Precision
precision_micro = precision_score(y_test, y_pred_classes, average='micro') # Micro average precision
precision_macro = precision_score(y_test, y_pred_classes, average='macro') # Macro average precision
print("Micro Average Precision:", precision_micro)
print("Macro Average Precision:", precision_macro)

# Recall
recall_micro = recall_score(y_test, y_pred_classes, average='micro') # Micro average recall
recall_macro = recall_score(y_test, y_pred_classes, average='macro') # Macro average recall
print("Micro Average Recall:", recall_micro)
print("Macro Average Recall:", recall_macro)

# F1 score
f1_micro = f1_score(y_test, y_pred_classes, average='micro') # Micro average F1 score
f1_macro = f1_score(y_test, y_pred_classes, average='macro') # Macro average F1 score
print("Micro Average F1 Score:", f1_micro)
print("Macro Average F1 Score:", f1_macro)


      #recall, micro and macro avg-precision and recall f1score

#compare all 3 models based on macro avg F1 SCORE





!ls

#Load previously trained LSTM Model
#from keras.models import load_model
#model_path ='./c1_lstm_model_acc_0.856.h5'
#pretrained_lstm_model = load_model(model_path)
#summarize model.
#pretrained_lstm_model.summary()

sample_reviews = pd.read_csv("a3_IMDb_Unseen_Reviews.csv")

sample_reviews.head(5)

unseen_reviews = sample_reviews['Review Text']

unseen_processed = []
for review in unseen_reviews:
  review = preprocess_text(review)
  unseen_processed.append(review)

# Tokenising instance with earlier trained tokeniser
unseen_tokenized = word_tokenizer.texts_to_sequences(unseen_processed)

# Pooling instance to have maxlength of 100 tokens
unseen_padded = pad_sequences(unseen_tokenized, padding='post', maxlen=maxlen)

unseen_sentiments = lstm_model.predict(unseen_padded)

unseen_sentiments

sample_reviews['Predicted Sentiments'] = np.round(unseen_sentiments*10,1)

df_prediction_sentiments = pd.DataFrame(sample_reviews['Predicted Sentiments'], columns = ['Predicted Sentiments'])
df_movie                 = pd.DataFrame(sample_reviews['Movie'], columns = ['Movie'])
df_review_text           = pd.DataFrame(sample_reviews['Review Text'], columns = ['Review Text'])
df_imdb_rating           = pd.DataFrame(sample_reviews['IMDb Rating'], columns = ['IMDb Rating'])


dfx=pd.concat([df_movie, df_review_text, df_imdb_rating, df_prediction_sentiments], axis=1)

dfx.to_csv("./c2_IMDb_Unseen_Predictions.csv", sep=',', encoding='UTF-8')

dfx.head(5)



##########################

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Dropout, Conv1D, GlobalMaxPooling1D, LSTM
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Define FNN model
fnn_model = Sequential()
embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)
fnn_model.add(embedding_layer)
fnn_model.add(Flatten())
fnn_model.add(Dense(512, activation='relu'))
fnn_model.add(Dropout(0.5))
fnn_model.add(Dense(1, activation='sigmoid'))
fnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define CNN model
cnn_model = Sequential()
embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)
cnn_model.add(embedding_layer)
cnn_model.add(Conv1D(128, 5, activation='relu'))
cnn_model.add(GlobalMaxPooling1D())
cnn_model.add(Dense(1, activation='sigmoid'))
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define LSTM model
lstm_model = Sequential()
embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)
lstm_model.add(embedding_layer)
lstm_model.add(LSTM(128))
lstm_model.add(Dense(1, activation='sigmoid'))
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train FNN model
fnn_model_history = fnn_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)

# Train CNN model
cnn_model_history = cnn_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)

# Train LSTM model
lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)

# Evaluate FNN model
fnn_score = fnn_model.evaluate(X_test, y_test, verbose=1)
print("FNN Test Score:", fnn_score[0])
print("FNN Test Accuracy:", fnn_score[1])

# Evaluate CNN model
cnn_score = cnn_model.evaluate(X_test, y_test, verbose=1)
print("CNN Test Score:", cnn_score[0])
print("CNN Test Accuracy:", cnn_score[1])

# Evaluate LSTM model
lstm_score = lstm_model.evaluate(X_test, y_test, verbose=1)
print("LSTM Test Score:", lstm_score[0])
print("LSTM Test Accuracy:", lstm_score[1])

#Plot accuracy for all models
#plt.plot(fnn_model_history.history['accuracy'])
plt.plot(fnn_model_history.history['val_accuracy'])
#plt.plot(cnn_model_history.history['accuracy'])
plt.plot(cnn_model_history.history['val_accuracy'])
#plt.plot(lstm_model_history.history['accuracy'])
plt.plot(lstm_model_history.history['val_accuracy'])
plt.title('Model Test Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['FNN Test', 'CNN Test', 'LSTM Test'], loc='upper left')
plt.show()

# Plot loss for all models
#plt.plot(fnn_model_history.history['loss'])
plt.plot(fnn_model_history.history['val_loss'])
#plt.plot(cnn_model_history.history['loss'])
plt.plot(cnn_model_history.history['val_loss'])
#plt.plot(lstm_model_history.history['loss'])
plt.plot(lstm_model_history.history['val_loss'])
plt.title('Model Test Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['FNN Test', 'CNN Test', 'LSTM Test'], loc='upper left')
plt.show()  #only test

from sklearn.metrics import f1_score

# Define functions to evaluate model and calculate F1 score
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_classes = (y_pred > 0.5).astype(int)
    f1 = f1_score(y_test, y_pred_classes, average='macro')
    return f1

# Evaluate FNN model
fnn_f1 = evaluate_model(fnn_model, X_test, y_test)

# Evaluate CNN model
cnn_f1 = evaluate_model(cnn_model, X_test, y_test)

# Evaluate LSTM model
lstm_f1 = evaluate_model(lstm_model, X_test, y_test)

# Print F1 scores for comparison
print("FNN Macro Average F1 Score:", fnn_f1)
print("CNN Macro Average F1 Score:", cnn_f1)
print("LSTM Macro Average F1 Score:", lstm_f1)

# Compare the models based on F1 scores
if fnn_f1 > cnn_f1 and fnn_f1 > lstm_f1:
    print("FNN model has the highest macro average F1 score.")
elif cnn_f1 > fnn_f1 and cnn_f1 > lstm_f1:
    print("CNN model has the highest macro average F1 score.")
elif lstm_f1 > fnn_f1 and lstm_f1 > cnn_f1:
    print("LSTM model has the highest macro average F1 score.")
else:
    print("There is a tie between two or more models.")